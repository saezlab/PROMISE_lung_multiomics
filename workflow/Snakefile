# To run snakemake -s Snakefile --profile profile/config.yaml 
#Run 'motus downloadDB' before using the motus profiler

nThreads   = 8

# import timedelta
from datetime import timedelta

dir_input  = "/Users/ecekartal/Documents/SAEZ/projects/PROMISE_lung_multiomics/input"
dir_output = "/Users/ecekartal/Documents/SAEZ/projects/PROMISE_lung_multiomics/output"

# We need to specify all unique samples here once (later this goes into a config file)
# Dirty approach here: Use all files from particular directory to colelct sample names
allSamplesUnique, = glob_wildcards("/Users/ecekartal/Documents/SAEZ/projects/PROMISE_lung_multiomics/input/rawdata/{id}_1_sequence.txt.gz")

# Append to this list all files you want to have in the end (it is enough to specify the output files from the last rule usually)
allResultFiles = []

motus = expand('{dir}/motus_v3.0.1/taxa/merged.taxa', 
    dir = dir_output, sample = allSamplesUnique)
allResultFiles.extend(motus)

motus_allBAMs = expand('{dir}/motus_v3.0.1/snv/motus_snv', 
    dir = dir_output, sample = allSamplesUnique)
allResultFiles.extend(motus_allBAMs)

# Rules that can run locally and do not need a designated cluster submission
localrules: all

# Update the allResultFiles array if you need additional output files outside of the normal dependency chain
rule all:
    input: allResultFiles

####################################################################################################

# 1. Read QC (FastQC)
# https://snakemake-wrappers.readthedocs.io/en/0.71.0/wrappers/fastqc.html

rule fastqc:
    input:
        forwardRead = dir_input + "/rawdata/{sample}_1_sequence.txt.gz",
        reverseRead = dir_input + "/rawdata/{sample}_2_sequence.txt.gz"
    output:
        html1       = dir_output + "/qc/fastqc/{sample}_1_sequence_fastqc.html",
        html2       = dir_output + "/qc/fastqc/{sample}_2_sequence_fastqc.html",
        zip1        = dir_output + "/qc/fastqc/{sample}_1_sequence_fastqc.zip",
        zip2        = dir_output + "/qc/fastqc/{sample}_2_sequence_fastqc.zip" 
        # the suffix _fastqc.zip is necessary for multiqc to find the file. 
        # If not using multiqc, you are free to choose an arbitrary filename
    params: 
        outdir      = dir_output + "/qc/fastqc"
    log: 
        dir_output + "/logs/fastqc/{sample}.log"
    message: 
        "Running fastQC for sample {wildcards.sample}"
    threads: 
        nThreads
    conda: 
        "envs/Preprocess.yaml"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"

    shell:
        """
        fastqc \
            {input} \
            -t {threads} \
            -o {params.outdir}
        """

####################################################################################################

# 2. Adapter clipping and merging (short-read: fastp)
# https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/fastp.html
rule fastp_pe:
    input:
        sample    = [dir_input + "/rawdata/{sample}_1_sequence.txt.gz", 
                     dir_input + "/rawdata/{sample}_2_sequence.txt.gz"],
        fastqc    = rules.fastqc.output # Not strictly required but makes sure the fastqc rule actually runs as otherwise, it would never run because no dependencies
    output:
        trimmed   = [dir_output + "/processed_data/trimmed/{sample}.1.fastq", 
                     dir_output + "/processed_data/trimmed/{sample}.2.fastq"],
        # Unpaired reads separately
        unpaired1 = dir_output + "/processed_data/trimmed/{sample}.u1.fastq",
        unpaired2 = dir_output + "/processed_data/trimmed/{sample}.u2.fastq",
        # or in a single file
        # unpaired="trimmed/pe/{sample}.singletons.fastq",
        merged    = dir_output + "/processed_data/trimmed/{sample}.merged.fastq",
        failed    = dir_output + "/processed_data/trimmed/{sample}.failed.fastq",
        html      = dir_output + "/report/{sample}.html",
        json      = dir_output + "/report/{sample}.json"
    message:
        "Running fastp for sample {wildcards.sample}"
    log:
        dir_output + "/logs/fastp/{sample}.log"
    params:
        adapters  = dir_input + "/adapt.fas", 
        extra     = "--merge"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),  
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    wrapper:
        "v1.19.1/bio/fastp"

####################################################################################################

# 3. Low complexity and quality filtering (short-read: bbduk)
# https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/bbtools/bbduk.html

rule bbduk_pe:
    input:
        sample    = [dir_output + "/processed_data/trimmed/{sample}.1.fastq", 
                     dir_output + "/processed_data/trimmed/{sample}.2.fastq"],
        adapters  = dir_input + "/adapt.fas"
    output:
        trimmed   = [dir_output + "/processed_data/filtered/{sample}.1.fastq", 
                     dir_output + "/processed_data/filtered/{sample}.2.fastq"],
        singleton = dir_output + "/processed_data/filtered/{sample}.single.fastq",
        discarded = dir_output + "/processed_data/filtered/{sample}.discarded.fastq",
        stats     = dir_output + "/processed_data/filtered/{sample}.stats.txt",
    log:
         dir_output + "/logs/bbduk_pe/{sample}.log"
    conda: 
        "envs/Preprocess.yaml"
    message:
        "Running bbduk for sample {wildcards.sample}"
    params:
        extra     = lambda w, input: "ref={},adapters,artifacts ktrim=r k=23 mink=11 hdist=1 tpe tbo trimpolygright=10 minlen=25 maxns=30 entropy=0.5 entropywindow=50 entropyk=5".format(input.adapters),
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"

    threads: 
        nThreads
    wrapper:
        "v1.20.0/bio/bbtools/bbduk"

####################################################################################################

# 4. Read QC (FastQC) after trimming

rule fastqcProcessed:
    input:
        forwardRead = dir_output + "/processed_data/filtered/{sample}.1.fastq",
        reverseRead = dir_output + "/processed_data/filtered/{sample}.2.fastq"
    output:
        html1       = dir_output + "/qc/fastqc_processed/{sample}.1_fastqc.html",
        html2       = dir_output + "/qc/fastqc_processed/{sample}.2_fastqc.html",
        zip1        = dir_output + "/qc/fastqc_processed/{sample}.1_fastqc.zip",
        zip2        = dir_output + "/qc/fastqc_processed/{sample}.2_fastqc.zip" 
        # the suffix _fastqc.zip is necessary for multiqc to find the file. 
        # If not using multiqc, you are free to choose an arbitrary filename
    params: 
        outdir      = dir_output + "/qc/fastqc_processed"
    log: 
        dir_output + "/logs/fastqc_processed/{sample}.log"
    message: 
        "Running fastQC after processing for sample {wildcards.sample}"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=80 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
        "envs/Preprocess.yaml"
    shell:
        """
        fastqc \
            {input} \
            -t {threads} \
            -o {params.outdir}
        """

####################################################################################################

# 5a. Bowtie2 mapping against host sequence database, keep both aligned and unaligned reads (paired-end reads)

rule human_filtering:
    input:
        fastq1 = dir_output + "/processed_data/filtered/{sample}.1.fastq",
        fastq2 = dir_output + "/processed_data/filtered/{sample}.2.fastq",
        fastqcProcessed = rules.fastqcProcessed.output # Not strictly required but makes sure the fastqc rule actually runs as otherwise, it would never run because no dependencies
    output:
        bam    = temp(dir_output + "/processed_data/bam/{sample}_mapped_and_unmapped.bam")
    log:
        dir_output + "/logs/bowtie/{sample}.log"
    message:
        "Running bowtie human mapping for sample {wildcards.sample}"
    params:
       hostDB  = dir_input + "/human_GRCh38/GRCh38_noalt_as"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 20000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=120 * attempt)),        
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"

    threads: 
        nThreads
    conda: 
        "envs/Preprocess.yaml"
    shell:
        """
        bowtie2 \
            -p {threads} -x {params.hostDB} \
            -1 {input.fastq1} \
            -2 {input.fastq2} 2> {log} | samtools view -bS - > {output.bam}
        """

####################################################################################################

# 6. Filter  bam files

rule samtools_view:
    input:
        bam         = rules.human_filtering.output.bam,
    output:
        unmappedBam = dir_output + "/processed_data/bam/{sample}.bothReadsUnmapped.bam",
        idx         = dir_output + "/processed_data/bam/{sample}.bai"
    log:
        dir_output + "/logs/samtools/{sample}.log"
    resources:
        mem_mb      =lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime     =lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=30 * attempt)),
        slurm_extra ="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"

    params:
        slurm_extra ="",  # optional params string
        region      ="",  # optional region string
    threads: 2
    wrapper:
        "v1.21.0/bio/samtools/view"

####################################################################################################

# 7. Convert a bam file with paired end reads back to unaligned reads in a two separate fastq files with samtools.

rule samtools_fastq_separate:
    input:
        unmappedBam  = dir_output + "/processed_data/bam/{sample}.bothReadsUnmapped.bam",
    output:
        hostremoved1 = dir_output + "/processed_data/bam/{sample}_host_removed_R1.fastq",
        hostremoved2 = dir_output + "/processed_data/bam/{sample}_host_removed_R2.fastq",
    log:
        dir_output + "/logs/samtools/{sample}.separate.log"
    resources:
        mem_mb      =lambda wildcards, input, threads, attempt: 10000 * attempt,
        runtime     =lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=90 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    params:
        sort="-m 4G",
        fastq="-n"
    threads:
        nThreads
    wrapper:
        "v1.21.0/bio/samtools/fastq/separate"


####################################################################################################

# 8. Read QC (FastQC) after trimming

rule fastqc_hostFil:
    input:
        forwardRead = rules.samtools_fastq_separate.output.hostremoved1,
        reverseRead = rules.samtools_fastq_separate.output.hostremoved2
    output:
        html1       = dir_output + "/qc/fastqc_hostFil/{sample}.1_fastqc.html",
        html2       = dir_output + "/qc/fastqc_hostFil/{sample}.2_fastqc.html",
        zip1        = dir_output + "/qc/fastqc_hostFil/{sample}.1_fastqc.zip",
        zip2        = dir_output + "/qc/fastqc_hostFil/{sample}.2_fastqc.zip" 
        # the suffix _fastqc.zip is necessary for multiqc to find the file. 
        # If not using multiqc, you are free to choose an arbitrary filename
    params: 
        outdir      = dir_output + "/qc/fastqc_hostFil"
    log: 
        dir_output + "/logs/fastqc_hostFil/{sample}.log"
    message: 
        "Running fastQC after host filtering for sample {wildcards.sample}"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 2000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
        "envs/Preprocess.yaml"
    shell:
        """
        fastqc \
            {input} \
            -t {threads} \
            -o {params.outdir}
        """

####################################################################################################

# 9. Taxonomic profiling by motus v3.0.1
rule motus_taxa:
    input:
        fastq1 = rules.samtools_fastq_separate.output.hostremoved1,
        fastq2 = rules.samtools_fastq_separate.output.hostremoved2,
        fastqc_hostFil = rules.fastqc_hostFil.output # Not strictly required but makes sure the fastqc rule actually runs as otherwise, it would never run because no dependencies

    output:
        motus  = dir_output + "/motus_v3.0.1/rel/{sample}.rel.motu",
        motu_c = dir_output + "/motus_v3.0.1/taxa/{sample}.motu",
        mgc = dir_output + "/motus_v3.0.1/mgc/{sample}.mgc"
    log:
        dir_output + "/logs/motus_v3.0.1/{sample}.motusV3_profile_taxa.log"
    message:
        "Running motus taxonomic profiling for sample {wildcards.sample}"
    params:
        # -n for output sample name
        # -M  save the marker gene cluster (MGC) counts
        # -g marker genes cutoff
        markerGenesCutoff = 3,
        # -l min read lenght
        minReadLength = 75,
        DBLoc = dir_input + "/db_mOTU"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 30000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=120 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
       "envs/Preprocess.yaml"
    shell:
    # Generating taxonomic profiles
        """
        motus profile \
            -f {input.fastq1} \
            -r {input.fastq2} \
            -o {output.motus} \
            -M {output.mgc} \
            -l {params.minReadLength} \
            -g {params.markerGenesCutoff} \
            -n {wildcards.sample} \
            -t {threads} \
            -db {params.DBLoc} &> {log} 

        motus profile \
            -f {input.fastq1} \
            -r {input.fastq2} \
            -o {output.motu_c} \
            -l {params.minReadLength} \
            -g {params.markerGenesCutoff} \
            -n {wildcards.sample} \
            -t {threads} \
            -c \
            -db {params.DBLoc} &> {log} 
        """

####################################################################################################

# 10. Merge motus files
rule motus_merge:
    input: 
        allmotu    = expand('{dir}/motus_v3.0.1/taxa/{sample}.motu', 
        dir = dir_output, sample = allSamplesUnique)
    output:
        mergedMotu = dir_output + "/motus_v3.0.1/taxa/merged.taxa"
    log:
        dir_output + "/logs/motus_v3.0.1/motus_v3_merged.log"
    message:
        "Running motus merge for samples"
    params:
        # -d Call metaSNV on all bam files in the directory. [Mandatory]
        dirInput  = dir_output + "/motus_v3.0.1/taxa/",
        DBLoc = dir_input + "/db_mOTU"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 5000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
       "envs/Preprocess.yaml"
    shell:
    # Generating taxonomic profiles
        """
        motus merge \
            -d {params.dirInput} \
            -t {threads} \
            -db {params.DBLoc} \
            -o {output.mergedMotu} &> {log} 
        """

####################################################################################################

# 11. Generating single nucleotide variant (SNV) profiles using MGs 
rule motus_map_snv:
    input:
        fastq1 = rules.samtools_fastq_separate.output.hostremoved1,
        fastq2 = rules.samtools_fastq_separate.output.hostremoved2,
    output:
        bam    = dir_output + "/motus_v3.0.1/snv/bam/{sample}.bam",
    log:
        dir_output + "/logs/motus_v3.0.1/{sample}.motus_v3_map_snv.log"
    message:
        "Running motus merge for samples"
    params:
        DBLoc = dir_input + "/db_mOTU"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 30000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=120 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
       "envs/Preprocess.yaml"
    shell:
        """
        # Generating single nucleotide variant (SNV) profiles using MGs
        # map_snv takes one or multiple sequencing files and aligns reads against the mOTU profiler database:
        motus map_snv \
            -f {input.fastq1} \
            -r {input.fastq2} \
            -l 75 \
            -t {threads} \
            -o {output.bam} \
            -db {params.DBLoc} &> {log} 

        """
 # samtools view \
        #     -b {output.sam} - > {output.bam}
####################################################################################################

# 12. Generating single nucleotide variant (SNV) profiles using MGs 
rule motus_snv_call:
    input: 
        allbam    = expand('{dir}/motus_v3.0.1/snv/bam/{sample}.bam', 
        dir = dir_output, sample = allSamplesUnique)
    output:
        motus_snv = directory(dir_output + "/motus_v3.0.1/snv/motus_snv")
    log:
        dir_output + "/logs/motus_v3.0.1/motus_v3_snv_call.log"
    message:
        "Running motus merge for samples"
    params:
        # -d Call metaSNV on all bam files in the directory. [Mandatory]
        bam       = dir_output + "/motus_v3.0.1/snv/bam",
        DBLoc = dir_input + "/db_mOTU"
    resources:
        mem_mb=lambda wildcards, input, threads, attempt: 30000 * attempt,
        runtime=lambda wildcards, input, threads, attempt: str(
            timedelta(minutes=60 * attempt)),
        slurm_extra="--mail-type=FAIL --mail-user=ece.kartal@uni-heidelberg.de"
    threads: 
        nThreads
    conda: 
       "envs/Preprocess.yaml"
    shell:
        """
        # snv_call calls variants using the metaSNV package
        motus snv_call \
            -d {params.bam} \
            -db {params.DBLoc} \
            -o {output.motus_snv} &> {log} 
        """



# The quality of assembly can be assessed by tools such as MetaQUAST
        #sam    = dir_output + "/motus_v3.0.1/meta_snv/sam/{sample}.sam",


# #Perform optional post-processing with:
# #bracken
# #Standardises output tables
# #Present QC for raw reads (MultiQC)
# #Plotting Kraken2, Centrifuge, Kaiju and MALT results (Krona)

